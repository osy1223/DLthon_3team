{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c249b9e7",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df13e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from kiwipiepy.utils import Stopwords\n",
    "kiwi=Kiwi()\n",
    "stopwords =Stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6276ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80af4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d33cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43a6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a388b",
   "metadata": {},
   "source": [
    "# Part | 데이터 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756d54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/aiffel/dktc/data_forder/Final_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea05e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.kiwi = Kiwi(typos='basic_with_continual_and_lengthening')\n",
    "        self.stopwords = Stopwords()\n",
    "        \n",
    "    # 데이터 정제 (불필요한 문자, 특수 문자, 이메일, URL, 자음/모음 등 제거)\n",
    "    def clean_text(self, text):\n",
    "        # E-mail 제거\n",
    "        text = re.sub(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)', '', text)\n",
    "        # URL 제거\n",
    "        text = re.sub(r'(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text)\n",
    "        # 한글 자음, 모음 제거\n",
    "        text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ]+)', '', text)\n",
    "        # 알파벳, 숫자 제거\n",
    "        text = re.sub(r'([a-zA-Z0-9]+)', '', text)\n",
    "        # HTML 태그 제거\n",
    "        text = re.sub(r'<[^>]*>', '', text)\n",
    "        # 특수 기호 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # 한글과 공백을 제외한 모든 문자 제거\n",
    "        text = re.sub(r'[^가-힣\\s]', '', text)\n",
    "        # 다중 공백을 단일 공백으로\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    # 형태소 분석 및 불용어 제거\n",
    "    def analyze_morphs(self, text):\n",
    "        # 형태소 분석 수행 및 불용어 제거\n",
    "        tokens = [(token.form,token.tag )for token in self.kiwi.tokenize(text, stopwords=self.stopwords)]\n",
    "        return tokens\n",
    "    \n",
    "    # 'EF' 태그가 있는 형태소 제거\n",
    "    def filter_ef_tag(self, tokens):\n",
    "        # 'EF' 품사 태그가 있는 형태소 제거\n",
    "        filtered_tokens = [token[0] for token in tokens if token[1] != 'EF']\n",
    "        return filtered_tokens\n",
    "\n",
    "    # 추가적인 단어 필터링\n",
    "    def filter_specific_words(self, tokens):\n",
    "        words_to_remove = ['ㅂ니다', '안', '야', '너', '키키', '키', '거', 'ㄴ가요', '습니다']\n",
    "        filtered_tokens = [token for token in tokens if token not in words_to_remove]\n",
    "        return filtered_tokens\n",
    "\n",
    "    # 전체 과정 통합 (형태소 분석, EF 품사 및 불용어 제거)\n",
    "    def process(self, text):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.analyze_morphs(cleaned_text)\n",
    "        tokens_without_ef = self.filter_ef_tag(tokens)\n",
    "        filtered_tokens = self.filter_specific_words(tokens_without_ef)\n",
    "        return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TextProcessor()\n",
    "\n",
    "data['ppc'] = data['conversation'].apply(lambda text: processor.process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450656d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['conversation','ppc']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8659f",
   "metadata": {},
   "source": [
    "# Part | 빈도수 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나눔고딕 폰트 설치\n",
    "!apt-get install -y fonts-nanum\n",
    "\n",
    "# 설치된 나눔고딕 폰트를 matplotlib에 적용하기 위한 코드입니다.\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 나눔고딕 폰트 경로를 가져와서 matplotlib에 적용\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=fontprop.get_name())  # 폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스별로 형태소 빈도 계산 및 시각화 준비\n",
    "def visualize_class_wordcloud(df):\n",
    "    classes = df['class'].unique()\n",
    "    \n",
    "    for class_name in classes:\n",
    "        # 해당 클래스에 속하는 문장의 형태소 추출\n",
    "        class_morphs = df[df['class'] == class_name]['ppc'].sum()  # 모든 형태소 리스트를 합침\n",
    "        \n",
    "        # 형태소 빈도 계산\n",
    "        word_freq = Counter(class_morphs)\n",
    "        \n",
    "        # Word Cloud 생성\n",
    "        wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', \n",
    "                              width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "        \n",
    "        # Word Cloud 시각화\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for {class_name}')\n",
    "        plt.show()\n",
    "\n",
    "# WordCloud 시각화 실행\n",
    "visualize_class_wordcloud(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스별로 형태소 빈도 계산 및 시각화 준비\n",
    "def visualize_class_barplot(df):\n",
    "    classes = df['class'].unique()\n",
    "    \n",
    "    for class_name in classes:\n",
    "        # 해당 클래스에 속하는 문장의 형태소 추출\n",
    "        class_morphs = df[df['class'] == class_name]['ppc'].sum()  # 모든 형태소 리스트를 합침\n",
    "        \n",
    "        # 형태소 빈도 계산\n",
    "        word_freq = Counter(class_morphs)\n",
    "        \n",
    "        # 상위 20개의 형태소만 추출\n",
    "        most_common_20 = word_freq.most_common(20)\n",
    "        \n",
    "        # 형태소와 빈도 리스트 분리\n",
    "        words, counts = zip(*most_common_20)\n",
    "        \n",
    "        # 바 그래프 생성\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(words, counts, color='skyblue')\n",
    "        plt.xlabel('빈도수')\n",
    "        plt.ylabel('형태소')\n",
    "        plt.title(f'{class_name} - 상위 500개 형태소 빈도수', fontproperties=fontprop)\n",
    "        plt.gca().invert_yaxis()  # 가장 빈도가 높은 단어가 상단에 오도록\n",
    "        plt.show()\n",
    "\n",
    "# 바 그래프 시각화 실행\n",
    "visualize_class_barplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fccd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_mapping = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화' : 4\n",
    "}\n",
    "df['class'] = df['class'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['ppc'])\n",
    "\n",
    "# 시퀀스 인코딩\n",
    "sequences = tokenizer.texts_to_sequences(df['ppc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(sequence_lengths, vert=False)\n",
    "plt.title('Boxplot of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9923775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 시퀀스 길이 결정 (% 로 결정)\n",
    "max_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용\n",
    "X = pad_sequences(sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# 레이블 준비\n",
    "y = df['class'].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
