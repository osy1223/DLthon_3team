{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7d975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RuntimeError: CUDA error: device-side assert triggered\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
    "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
    "'''\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ca8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adamp in /opt/conda/lib/python3.9/site-packages (0.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6b328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004a2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '/aiffel/aiffel/dktc/add_data/result.csv'\n",
    "train = pd.read_csv(train_data)\n",
    "\n",
    "test_data = '/aiffel/aiffel/dktc/add_data/test.csv'\n",
    "test = pd.read_csv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f90e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [(train['class']== \"갈취 대화\"), \n",
    "         (train['class']== \"기타 괴롭힘 대화\"), \n",
    "         (train['class']== \"일반 대화\"),\n",
    "         (train['class']== \"직장 내 괴롭힘 대화\"),\n",
    "         (train['class']== \"협박 대화\")]\n",
    "choicelist1 = [0,1,2,3,4]\n",
    "train['class']=np.select(list1, choicelist1)\n",
    "\n",
    "train=train[['conversation','class']]\n",
    "test=test[['conversation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294dfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4512697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRAINDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "        print(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, 0:2].values\n",
    "        sentence1 = row[0]\n",
    "#         sentence2 = row[1]\n",
    "        y = row[1]\n",
    "        inputs = self.tokenizer(\n",
    "            sentence1,\n",
    "#             sentence2,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=100\n",
    "        )\n",
    "\n",
    "        input_ids = torch.from_numpy(np.asarray(inputs['input_ids']))\n",
    "        attention_mask = torch.from_numpy(np.asarray(inputs['attention_mask']))\n",
    "\n",
    "        return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a90041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TESTDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "        print(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, 0:1].values\n",
    "        sentence1 = row[0]\n",
    "#         sentence2 = row[1]\n",
    "        inputs = self.tokenizer(\n",
    "            sentence1,\n",
    "#             sentence2,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=100\n",
    "        )\n",
    "\n",
    "        input_ids = torch.from_numpy(np.asarray(inputs['input_ids']))\n",
    "        attention_mask = torch.from_numpy(np.asarray(inputs['attention_mask']))\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69026846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465db858",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c09becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "epochs = 5\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34fb585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 검증\n",
    "def training(train_dataset,val_dataset, fold):\n",
    "    best_acc = 0\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-large\", num_labels=5).to(device)\n",
    "\n",
    "    dataset_train = TRAINDataset(train_dataset)\n",
    "    dataset_val = TRAINDataset(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = AdamP(model.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-2)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    # 스케줄러\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                              num_warmup_steps = 0,\n",
    "                                              num_training_steps = total_steps)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        model.train()\n",
    "        for batch_id, (token_ids, attention_masks, label) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            token_ids = token_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(token_ids, attention_masks)[0]\n",
    "            loss = F.cross_entropy(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_acc += calc_accuracy(out, label)\n",
    "\n",
    "        print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "\n",
    "        model.eval()\n",
    "        for batch_id, (token_ids, attention_masks, label) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            token_ids = token_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(token_ids, attention_masks)[0]\n",
    "            valid_acc += calc_accuracy(out, label)\n",
    "        print(\"epoch {} valid acc {}\".format(e+1, valid_acc / (batch_id+1)))\n",
    "            #    if valid_acc > best_acc:\n",
    "            #      torch.save(model, '/content/drive/MyDrive/한국어 문장 관계 분류 경진대회/open/model'+str(fold)+'.pt')\n",
    "        torch.save(model, '/aiffel/aiffel/dktc/add_data/model'+str(fold)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34532523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차검증\n",
    "def main():\n",
    "    seed= 2021 # 재현성을 위한 시드값 고정\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "\n",
    "    # kfold\n",
    "    kfold=[]\n",
    "    # StratifiedKFold : 불균형한 분포도를 가진 레이블 데이터 집합을 위한 KFold 방식\n",
    "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "    for train_idx, val_idx in splitter.split(train.iloc[:, 0:1],train.iloc[:, 1]):\n",
    "        kfold.append((train.iloc[train_idx,:],train.iloc[val_idx,:]))\n",
    "\n",
    "    for fold,(train_datasets, valid_datasets) in enumerate(kfold):\n",
    "        print(f'fold{fold} 학습중...')\n",
    "        training(train_dataset=train_datasets,val_dataset=valid_datasets,fold=fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592094e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0 학습중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           conversation  class\n",
      "0     지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...      4\n",
      "1     길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...      4\n",
      "2     너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...      1\n",
      "5     나 이틀뒤에 가나다 음식점 예약좀 해줘. 저녁7시로.\\n가나다 음식점이요.?\\n응....      3\n",
      "6     35번 손님 아이스커피 두잔나왔습니다\\n아이스커피? \\n네 맛있게드세요\\n저기요 아...      1\n",
      "...                                                 ...    ...\n",
      "4945  한달살기로 가보고 싶은 지역이 있어?\\n 우리나라 중에서 말하는거야?\\n 국내, 해...      2\n",
      "4946  그런 스트레스는 키키 재밌는 드라마나 영화나 예능이 최고지!\\n 난 요즘 갯마을 차...      2\n",
      "4947  나 저번에 소개팅했던 남자한테 연락이 왔어\\n 그래? 뭐라고 왔니?\\n 궁금하다 저...      2\n",
      "4948  우리나라 자국산 전투기 만드는거 알아?\\n 전투기를 왜 만드는거야?\\n 나라를 지키...      2\n",
      "4949  그래도 출퇴근이라도 편하면 좋지 키키\\n 뭐... 그게 나쁜 건 아니지...\\n 회...      2\n",
      "\n",
      "[3960 rows x 2 columns]\n",
      "                                           conversation  class\n",
      "3     어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...      0\n",
      "4     저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...      0\n",
      "14    김사원\\n대리님 왜 그러세요?\\n이거 오늘까지 제출해야 하는데 좀 해줄래?\\n네? ...      3\n",
      "17    얼마 가져왔어?\\n십만원 정도.\\n야 이걸로 새 폰을 어떻게 사\\n이게 최선이었어....      0\n",
      "22    어유 파리꼬여 어디서 계란썩는 냄새가 나냐 설마 사람한태 나는 냄새는 아닐테고\\n너...      1\n",
      "...                                                 ...    ...\n",
      "4913  오늘 케이뱅크 마비 됐었데\\n 아 업비트때문에?\\n 응 업비트에서 새로운 코인 상장...      2\n",
      "4920  대학교 다닐 때 알바 해본 적 있어?\\n 많이 했지. 기억이 남는 건 도미노피자\\n...      2\n",
      "4932  너네 집 평수가 혹시 몇 평이야?\\n 지금 내가 살고 있는 집 얘기하는 거야?\\n ...      2\n",
      "4941  뒷 주머니에 휴대폰 꽂아놓은 거 까먹고 화장실 갔다가 변기에 폰 빠졌어ㅠㅠ 당황스럽...      2\n",
      "4944  너희 형제 자매는 모범생이었음? 키키\\n 흠 성적과는 별개로 모범생이었죠 키키\\n ...      2\n",
      "\n",
      "[990 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc37e1057684887a3270a2b7b03961f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train acc 0.8409090909090909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e8d1cf917a4108a2b9f0e794887bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 valid acc 0.8971774193548387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3dc476a5d4482b95d384dc1d9acc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main() #학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 \n",
    "def inference(model, dataset_test):\n",
    "    test_dataset = TESTDataset(dataset_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    output_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (token_ids, attention_masks) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            attention_masks = attention_masks.long().to(device)\n",
    "            output=model(token_ids, attention_masks)[0]\n",
    "            logits = torch.nn.functional.softmax(output, dim=1).detach().cpu().numpy()\n",
    "            output_pred.extend(logits)\n",
    "    return output_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "list1 = [(train['class']== \"갈취 대화\"), \n",
    "         (train['class']== \"기타 괴롭힘 대화\"), \n",
    "         (train['class']== \"일반 대화\"),\n",
    "         (train['class']== \"직장 내 괴롭힘 대화\"),\n",
    "         (train['class']== \"협박 대화\")]\n",
    "choicelist1 = [0,1,2,3,4]\n",
    "'''\n",
    "label_dict = {\"갈취 대화\" : 0, \n",
    "              \"기타 괴롭힘 대화\" : 1, \n",
    "              \"일반 대화\" : 2,\n",
    "              \"직장 내 괴롭힘 대화\" : 3,\n",
    "              \"협박 대화\" : 4                            \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 도출\n",
    "def inference_main():\n",
    "    res = np.zeros((len(test),3)) \n",
    "    for i in range(5): \n",
    "        print(f'fold{i} 모델 추론중...')\n",
    "        # load my model\n",
    "        model = torch.load('/aiffel/aiffel/dktc/add_data/model'+str(i)+'.pt')\n",
    "\n",
    "        pred_answer = inference(model, test)\n",
    "\n",
    "        res += np.array(pred_answer) / 5 \n",
    "\n",
    "        ans= np.argmax(res, axis=-1)\n",
    "        out = [list(label_dict.keys())[_] for _ in ans]\n",
    "        submission[\"label\"] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be32453",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3436e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e87648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "tokenizer(\"힛걸 진심 최고로 멋지다.\", \"힛걸 진심 최고다 그 어떤 히어로보다 멋지다\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
