{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c96bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import MeCab\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 최적화\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 로드\n",
    "path = '~/aiffel/dktc/data_forder/result_kakao.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab 형태소 분석기 초기화\n",
    "mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519571b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab의 parse 결과를 파싱하여 (word, pos) 튜플의 리스트로 변환하는 함수\n",
    "def parse_mecab_output(text):\n",
    "    parsed = mecab.parse(text)\n",
    "    parsed_lines = parsed.strip().split('\\n')\n",
    "    tokens = []\n",
    "    for line in parsed_lines:\n",
    "        if line == 'EOS':\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                surface, feature = line.split('\\t')\n",
    "                pos = feature.split(',')[0]\n",
    "                tokens.append((surface, pos))\n",
    "            except ValueError:\n",
    "                continue  # 빈 줄이나 형식에 맞지 않는 줄은 무시\n",
    "    return tokens\n",
    "\n",
    "# 추가된 전처리 함수들\n",
    "def remove_urls_emails(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cba060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 및 형태소 분석 클래스 정의\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, pos_to_keep=None):\n",
    "        if pos_to_keep is None:\n",
    "            self.pos_to_keep = [\n",
    "                'NNG', 'NNP', 'NNB',  # 명사 계열\n",
    "                'VV', 'VA',            # 동사, 형용사\n",
    "                'MM', 'MAG',           # 관형사, 부사\n",
    "                'XPN', 'XSN', 'XSV', 'XSA' # 파생 접미사, 접두사\n",
    "            ]\n",
    "        else:\n",
    "            self.pos_to_keep = pos_to_keep\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        text = remove_urls_emails(text)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)  # 반복 문자 줄이기\n",
    "        text = re.sub(r'\\s+', ' ', text)             # 다중 공백 제거\n",
    "        text = text.strip()                          # 앞뒤 공백 제거\n",
    "        return text\n",
    "\n",
    "    def filter_by_pos(self, sentence):\n",
    "        parsed_sentence = parse_mecab_output(sentence)\n",
    "        filtered_tokens = [word for word, pos in parsed_sentence if pos in self.pos_to_keep]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def preprocess_and_tokenize(self, sentence):\n",
    "        if pd.isnull(sentence):\n",
    "            return []\n",
    "        sentence = self.normalize_text(sentence)\n",
    "        tokens = self.filter_by_pos(sentence)\n",
    "        return tokens\n",
    "\n",
    "    def apply_to_dataframe(self, df, column_name):\n",
    "        df[column_name] = df['conversation'].apply(self.preprocess_and_tokenize)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3600a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 클래스 인스턴스 생성\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# 데이터프레임 복사\n",
    "df = data.copy()\n",
    "\n",
    "df = df.iloc[:6950]\n",
    "\n",
    "# 전처리 및 토큰화 적용\n",
    "df = preprocessor.apply_to_dataframe(df, 'preprocess_conversation')\n",
    "\n",
    "# 토큰 리스트를 문자열로 변환하여 'processed_text' 컬럼 생성\n",
    "df['processed_text'] = df['preprocess_conversation'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af239298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "label_encoding = {\n",
    "    '협박 대화': 0,\n",
    "    '기타 괴롭힘 대화': 1,\n",
    "    '갈취 대화': 2,\n",
    "    '직장 내 괴롭힘 대화': 3,\n",
    "    '일반 대화' : 4\n",
    "}\n",
    "df['class'] = df['class'].map(label_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# 사전 훈련된 한국어 BERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "bert_model = TFBertModel.from_pretrained('kykim/bert-kor-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2760a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 길이 박스플롯을 위해 패딩 전에 시퀀스 길이 계산\n",
    "sequence_lengths = df['preprocess_conversation'].apply(len).tolist()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(sequence_lengths, vert=False)\n",
    "plt.title('Boxplot of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수 정의 (패딩 전에 시퀀스 길이 확인 완료)\n",
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',  # 시퀀스를 max_length로 패딩\n",
    "        truncation=True,       # max_length를 초과하는 시퀀스는 자름\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'    # 텐서 형식으로 반환\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5add93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 시퀀스 길이 결정\n",
    "max_seq_len = 200  # 메모리 부족 문제 해결을 위해 시퀀스 길이 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83898664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 수행: 'processed_text' 컬럼 사용\n",
    "tokenized_data = tokenize_texts(df['processed_text'], tokenizer, max_seq_len)\n",
    "\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "# 레이블 준비\n",
    "y = df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ff721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# input_ids와 attention_mask를 넘파이 배열로 변환 (이전 코드의 부분)\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_mask_np = attention_mask.numpy()\n",
    "\n",
    "# 학습과 검증 데이터를 8:2로 나눔 (클래스 비율 유지)\n",
    "X_train_input_ids, X_val_input_ids, X_train_attention_mask, X_val_attention_mask, y_train, y_val = train_test_split(\n",
    "    input_ids_np,       # input_ids\n",
    "    attention_mask_np,  # attention_mask\n",
    "    y,                  # target (class labels)\n",
    "    test_size=0.2,      # 검증 데이터 비율 20%\n",
    "    random_state=100,   # 시드 고정\n",
    "    stratify=y          # 클래스 비율 유지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "\n",
    "def build_classification_model(max_seq_len, num_classes):\n",
    "    # 입력 레이어 정의\n",
    "    input_ids = Input(shape=(max_seq_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_seq_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # BERT 모델을 통해 임베딩 획득\n",
    "    bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = bert_outputs.pooler_output  # [CLS] 토큰의 출력\n",
    "    \n",
    "    # 드롭아웃 레이어\n",
    "    dropout = Dropout(0.3)(pooled_output)\n",
    "    \n",
    "    # 출력 레이어\n",
    "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "    \n",
    "    # 모델 정의\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5201e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 수 설정\n",
    "num_classes = 5  # 필요에 따라 조정\n",
    "\n",
    "# 모델 생성\n",
    "model = build_classification_model(max_seq_len, num_classes)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 요약 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# 체크포인트 저장 디렉토리 생성\n",
    "checkpoint_dir = 'checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# 조기 종료 및 모델 체크포인트 콜백 설정\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),  # 체크포인트 저장 경로\n",
    "    monitor='val_loss',                                      # 모니터링할 지표\n",
    "    save_best_only=True,                                     # 최상의 모델만 저장\n",
    "    save_weights_only=True,                                  # 가중치만 저장\n",
    "    verbose=1                                                # 저장 시 메시지 출력\n",
    ")\n",
    "\n",
    "# 추가적인 학습률 스케줄러 (선택 사항)\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 콜백 리스트에 추가\n",
    "callbacks = [early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    [X_train_input_ids, X_train_attention_mask],\n",
    "    y_train,\n",
    "    validation_data=([X_val_input_ids, X_val_attention_mask], y_val),\n",
    "    epochs=10,\n",
    "    batch_size=16, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f168d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터에 대한 예측 및 분류 보고서\n",
    "y_pred = model.predict([X_val_input_ids, X_val_attention_mask])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "report = classification_report(y_val, y_pred_classes, target_names=label_encoding.keys(), output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df = report_df[['precision', 'recall', 'f1-score', 'support']]\n",
    "report_df = report_df.round(4)\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08388eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 저장된 모델의 가중치를 불러오는 경로 설정\n",
    "model_path = os.path.join('checkpoints', 'best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d281fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 매핑 (번호 -> 텍스트)\n",
    "label_mapping = {\n",
    "    0: '협박 대화',\n",
    "    1: '기타 괴롭힘 대화',\n",
    "    2: '갈취 대화',\n",
    "    3: '직장 내 괴롭힘 대화',\n",
    "    4: '일반 대화'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89b5b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 저장된 모델 불러오기\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c78d1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 테스트 데이터 로드\n",
    "test_data_path = '~/aiffel/dktc/data_forder/test.csv'\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5dbbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 테스트 데이터 전처리 (훈련 데이터와 동일한 방식으로)\n",
    "test_df['preprocess_conversation'] = test_df['text'].apply(preprocessor.preprocess_and_tokenize)\n",
    "test_df['processed_text'] = test_df['preprocess_conversation'].apply(lambda tokens: ' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f9f01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 토큰화된 테스트 데이터 생성\n",
    "tokenized_test_data = tokenize_texts(test_df['processed_text'], tokenizer, max_seq_len)\n",
    "input_ids_test = tokenized_test_data['input_ids']\n",
    "attention_mask_test = tokenized_test_data['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0096a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델로 예측 수행\n",
    "y_test_pred = model.predict([input_ids_test, attention_mask_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "464b3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 예측 클래스 추출\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ddc105ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold 값 설정\n",
    "threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c342f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 클래스 값 (threshold 적용 후): [2 3 3 1 1 0 3 2 4 2 0 4 3 3 3 0 4 1 0 1 0 1 1 2 2 3 1 4 3 1 2 0 4 2 0 1 1\n",
      " 2 0 2 0 2 3 2 4 3 0 1 4 0 2 1 2 3 3 1 2 4 1 1 2 1 3 1 3 0 1 1 0 3 0 3 1 3\n",
      " 0 3 3 4 3 0 2 0 0 0 1 3 2 1 2 3 3 3 0 4 0 3 1 1 2 3 2 2 1 0 3 3 3 1 1 2 2\n",
      " 0 2 3 2 2 2 3 3 2 3 1 2 2 2 2 3 4 2 1 2 1 3 1 2 0 4 0 2 3 0 1 2 1 3 0 0 0\n",
      " 0 2 0 1 0 0 3 1 0 3 1 1 0 1 4 0 3 1 3 4 1 2 0 3 3 1 4 1 3 3 2 2 3 0 3 3 0\n",
      " 3 3 0 2 0 0 1 2 1 2 1 0 3 1 1 3 0 4 1 0 1 3 2 3 3 0 2 0 3 2 1 1 4 0 0 2 3\n",
      " 1 4 0 4 3 0 2 0 3 0 1 1 2 1 1 2 0 1 1 0 1 3 4 2 3 1 3 0 0 1 0 3 3 2 0 3 2\n",
      " 3 2 0 1 1 3 1 1 4 1 2 2 1 0 3 2 3 3 1 3 3 2 0 1 3 3 3 2 3 2 1 4 2 1 1 3 1\n",
      " 4 1 2 0 3 1 0 3 2 2 0 3 2 3 2 2 4 1 3 4 2 1 1 0 3 0 0 2 2 2 2 4 3 1 2 0 0\n",
      " 1 2 4 2 2 4 1 0 3 0 0 2 3 1 2 0 0 0 1 2 2 2 1 1 1 0 2 1 4 2 2 3 4 1 3 0 2\n",
      " 2 4 0 3 3 4 2 1 2 2 3 0 3 3 1 2 1 0 0 1 2 0 0 3 2 3 0 2 2 0 1 1 3 3 1 0 1\n",
      " 2 3 2 0 4 0 3 3 1 1 2 2 3 1 0 0 1 1 2 2 3 3 0 1 1 2 0 2 1 0 3 4 3 1 0 3 2\n",
      " 2 3 0 3 3 2 2 0 3 1 3 2 3 0 2 2 3 2 3 3 3 2 3 3 4 0 3 2 0 1 3 3 2 2 1 3 0\n",
      " 1 2 0 2 1 1 4 4 0 1 0 0 2 2 3 3 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "# 7. 일반 대화에 대한 threshold 적용\n",
    "# 각 클래스의 확률에서 일반 대화의 확률(y_test_pred[:, 4])이 threshold 이상일 때, '일반 대화'로 강제 분류\n",
    "for i in range(len(y_test_pred)):\n",
    "    general_conversation_prob = y_test_pred[i, 4]  # 일반 대화의 확률\n",
    "    if general_conversation_prob >= threshold:\n",
    "        y_test_pred_classes[i] = 4  # 일반 대화로 분류\n",
    "        \n",
    "# 예측 클래스 값 확인 (디버깅용)\n",
    "print(\"예측 클래스 값 (threshold 적용 후):\", y_test_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "164a912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 예측 클래스 번호를 텍스트 레이블로 변환\n",
    "test_df['class'] = y_test_pred_classes\n",
    "test_df['class'] = test_df['class'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d403f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 제출 파일 작성\n",
    "\n",
    "class_to_target = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "\n",
    "# test_df에서 'class' 열의 값을 숫자로 변환하여 'target' 열 생성\n",
    "test_df['target'] = test_df['class'].map(class_to_target)\n",
    "\n",
    "# . 제출 파일 작성\n",
    "submission.columns = ['idx', 'target']\n",
    "submission = test_df[['idx', 'target']]\n",
    "submission.to_csv('~/aiffel/dktc/data_forder/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5bc7dd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '협박 대화', '일반 대화'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dcec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0c47d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    125\n",
       "1    120\n",
       "3    112\n",
       "0    106\n",
       "4     37\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecc015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
